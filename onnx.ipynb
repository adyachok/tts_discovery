{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import Wav2Vec2Processor, TFWav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import onnxruntime as rt\n",
    "import tf2onnx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 17:53:45.206338: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFWav2Vec2ForCTC.\n",
      "\n",
      "All the layers of TFWav2Vec2ForCTC were initialized from the model checkpoint at facebook/wav2vec2-base-960h.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2ForCTC for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = TFWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/Users/andreasgyascok/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n",
      "Loading cached processed dataset at /Users/andreasgyascok/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc/cache-0aee31f7b335be94.arrow\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "ds = ds.map(map_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "input_values = processor(ds[\"speech\"][1], return_tensors=\"tf\").input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 104560])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_values).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = tf.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([326])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOWING CLOTH THAT WAS THE ONLY GARMENT HE WORE\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription = processor.decode(predicted_ids[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_wav2vec2_for_ctc\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wav2vec2 (TFWav2Vec2MainLaye multiple                  94371712  \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "lm_head (Dense)              multiple                  24608     \n",
      "=================================================================\n",
      "Total params: 94,396,320\n",
      "Trainable params: 94,396,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 17:55:04.405611: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-31 17:55:04.445949: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/onnx_env/lib/python3.8/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 17:55:24.342254: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-31 17:55:30.773847: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 3179 nodes (-234), 4022 edges (-246), time = 3617.25903ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 48.184ms.\n",
      "  constant_folding: Graph size after: 3179 nodes (0), 4022 edges (0), time = 682.623ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 53.344ms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AUDIO_MAXLEN = 150000\n",
    "ONNX_PATH = \"onnx-wav2vec2-150k.onnx\"\n",
    "\n",
    "input_signature = (tf.TensorSpec((None, AUDIO_MAXLEN), tf.float32, name=\"speech\"),)\n",
    "_ = tf2onnx.convert.from_keras(model, input_signature=input_signature, output_path=ONNX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = rt.InferenceSession(ONNX_PATH)\n",
    "aspeech = np.array(ds[\"speech\"][1], dtype=np.float32)\n",
    "aspeech = np.expand_dims(aspeech, axis=0)\n",
    "padding = np.zeros((aspeech.shape[0], AUDIO_MAXLEN - aspeech.shape[1]), dtype=np.float32)\n",
    "speech = np.concatenate([aspeech, padding], axis=-1)\n",
    "onnx_outputs = session.run(None, {\"speech\": speech})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOWING CLOTH THAT WAS THE ONLY GARMENT HE WORE\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = np.argmax(onnx_outputs, axis=-1)[0]\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU tf2onnx onnxruntime\n",
    "!pip3 install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wav2vec2 import Wav2Vec2ForCTC\n",
    "\n",
    "model_id = \"vasudevgupta/gsoc-wav2vec2-960h\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_MAXLEN = 50000\n",
    "ONNX_PATH = \"onnx-wav2vec2.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signature = (tf.TensorSpec((None, AUDIO_MAXLEN), tf.float32, name=\"speech\"),)\n",
    "_ = tf2onnx.convert.from_keras(model, input_signature=input_signature, output_path=ONNX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/sample.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wav2vec2 import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(is_tokenizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"sample.wav\"\n",
    "\n",
    "speech, _ = sf.read(FILENAME)\n",
    "speech = tf.constant(speech, dtype=tf.float32)\n",
    "speech = processor(speech)[None]\n",
    "\n",
    "padding = tf.zeros((speech.shape[0], AUDIO_MAXLEN - speech.shape[1]))\n",
    "speech = tf.concat([speech, padding], axis=-1)\n",
    "speech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = rt.InferenceSession(ONNX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True)\n",
    "def jitted_forward(speech):\n",
    "    return model(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_outputs = session.run(None, {\"speech\": speech.numpy()})[0]\n",
    "tf_outputs = jitted_forward(speech)\n",
    "\n",
    "assert np.allclose(onnx_outputs, tf_outputs.numpy(), atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
    "prediction = np.argmax(onnx_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "print(\"prediction:\", prediction)\n",
    "Audio(filename=FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspeech = np.array(ds[\"speech\"][1], dtype=np.float32)\n",
    "aspeech = np.expand_dims(aspeech, axis=0)[:,:50000]\n",
    "onnx_outputs = session.run(None, {\"speech\": aspeech})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(onnx_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspeech = np.array(ds[\"speech\"][1], dtype=np.float32)\n",
    "aspeech = np.expand_dims(aspeech, axis=0)[:,:50000]\n",
    "aspeech[:, 25000:50000] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_outputs = session.run(None, {\"speech\": aspeech})[0]\n",
    "prediction = np.argmax(onnx_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspeech = np.array(ds[\"speech\"][1], dtype=np.float32)\n",
    "aspeech = np.expand_dims(aspeech, axis=0)[:,:40000]\n",
    "onnx_outputs = session.run(None, {\"speech\": aspeech})[0]\n",
    "prediction = np.argmax(onnx_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = np.array(ds[\"speech\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_length = []\n",
    "for i in ds[\"speech\"]:\n",
    "    speech_length.append(len(i))\n",
    "max(speech_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONNX Runtime",
   "language": "python",
   "name": "onnx_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
